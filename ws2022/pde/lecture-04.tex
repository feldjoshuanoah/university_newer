\section{Ritz-Galerkin methods}

These are pre-cursors of the finite element method.
We start in the simplified/abstract setting
\begin{quote}
    ``Find \(u \in V\) such that \(a\parentheses*{v, u} = \ell\parentheses*{v}\) for all \(v \in V\).''
\end{quote}
with some complete function space \(V\) with scalar product.
We choose a basis \(\braces*{\varphi_j}_{j \in \N}\) of \(V\) such that any \(u \in V\) has the representation
\[
    u\parentheses*{x} = \sum_{j = 1}^\infty \alpha_j \varphi_j\parentheses*{x},
\]
with \(\alpha_j \in \R\).


\subsection{Finite-dimensional subspaces}

The Ritz-Galerkin method ``discretizes'' \(u\) by a finite basis representation
\[
    u_N\parentheses*{x} = \sum_{j = 1}^N \alpha_j \varphi_j\parentheses*{x},
\]
with \(\alpha_j \in \R\) and a finite set of \(N\) basis functions, that is,
\[
    u_N \in V_N := \braces*{\varphi_j}_{j = 1, \ldots, N} \subset V
\]
with \(\dim\parentheses*{V_N} = N < \infty\).
With this we restrict the variational formulation to \(V_N\):
\begin{quote}
    ``Find \(u_N \in V_N\) such that \(a\parentheses*{u_N, v} = \ell\parentheses*{v}\) for all \(v \in V_N\).''
\end{quote}
The most important observations are:

\begin{remark}
    \begin{enumerate}
        \item Instead of using all \(v \in V\) it is sufficient to test with the basis elements \(\varphi_i, i = 1, \ldots, N\).
        With this the varational formulation becomes \(N\) independent equations.
        \item For the unknown solution \(u_N\) we make an approach based on the basis representation, that is, \(u_N = \sum_{j = 1}^N \alpha_j \varphi_j\parentheses*{x}\).
        The only remaining unknowns are \(N\) real values \(\alpha_j, j = 1, \ldots, N\).
    \end{enumerate}
\end{remark}

We find
\begin{align*}
    a\parentheses*{v, u_N} &= \ell\parentheses*{v} \quad \forall v \in V_N\\
    \iff a\parentheses*{\varphi_i, \sum_{j = 1}^N \alpha_j \varphi_j} &= \ell\parentheses*{\varphi_i} \quad \forall i = 1, \ldots, N,\\
    \iff \sum_{j = 1}^N a\parentheses*{\varphi_i, \varphi_j}\alpha_j &= \ell\parentheses*{\varphi_i} \quad \forall i = 1, \ldots, N,\\
    \iff \sum_{j = 1}^N a_{ij}\alpha_j &= b_i \quad \forall i = 1, \ldots, N,
\end{align*}
where we used linearity and defined the coefficients \(a_{ij} := a\parentheses*{\varphi_i, \varphi_j}\) and \(b_i = \ell\parentheses*{\varphi_i}\).
Combining those to a matrix and vector
\[
    A := \parentheses*{a_{ij}}_{i, j = 1, \ldots, N} \in \R^{N \times N}, \quad b = \parentheses*{b_i}_{i = 1, \ldots, N} \in \R^N
\]
together with the solution coefficients
\[
    x = \parentheses*{\alpha_i}_{i = 1, \ldots, N} \in \R^N
\]
we arrive at the linear system of equations
\[
    Ax = b
\]
to be solved for \(x = \parentheses*{\alpha_1, \ldots, \alpha_N}\), which gives the solution approximation
\[
    u_N = \alpha_1 \varphi_1\parentheses*{x} + \cdots + \alpha_N \varphi_N\parentheses*{x},
\]
based on the chosen basis.

\begin{theorem}
    Let \(a: V \times V \to \R\) be a continuous, coercive, symmetric bilinear form, and \(\ell: V \to \R\) a continuous, linear functional.
    Then we have
    \begin{enumerate}
        \item\label{theorem:4-2:1} The matrix \(A\) of the Ritz-Galerkin method is symmetric and positive definite.
        \item\label{theorem:4-2:2} The resulting linear system has a unique solution.
        \item\label{theorem:4-2:3} The Ritz-Galerkin solution satisfies
        \[
            \norm*{u_N}_V \le C\norm*{\ell}_{V'}
        \]
        with \(C\) independent of \(N\).
    \end{enumerate}
\end{theorem}

\begin{proof}
    For \ref{theorem:4-2:1} we consider for \(z \in \R^N\)
    \[
        z^T Az = \sum_{i, j = 1}^N z_i a_{ij}z_j = \sum_{i, j = 1}^N z_i a\parentheses*{\varphi_i, \varphi_j}z_j = a\parentheses*{\sum_{i = 1}^N z_i \varphi_i, \sum_{j = 1}^N z_j \varphi_j} = a\parentheses*{z_N, z_N}
    \]
    with \(z_N = \sum_{i = 1}^N z_i \varphi_i \in V_N\).
    Because \(z_N \in V\) holds and \(a\) is coercive, we have
    \[
        z^T Az = a\parentheses*{z_N, z_N} \ge \gamma\norm*{z_N}_V^2 > 0,
    \]
    hence, \(A\) is positive definite.
    Symmetry follows from the definition.
    For \ref{theorem:4-2:2} we conclude \(A\) is a regular matrix, because of \ref{theorem:4-2:1}.
    Using coercivity backwards we have
    \[
        \gamma\norm*{u_N}_V^2 \le a\parentheses*{u_N, u_N} = \ell\parentheses*{u_N} \le \norm*{\ell}_{V'}\norm*{u_N}V,
    \]
    which gives the statement of \ref{theorem:4-2:3} with \(C = \frac{1}{\gamma}\).
\end{proof}

\subsection{Concrete examples}

For clarification of the methods we consider the (rather trivial) case \(n = 1\) and \(\Omega = \brackets*{0, 1}\).
The Poisson problem then reads
\begin{align*}
    u'' &= f, \quad \text{in }\Omega,\\
    u\parentheses*{0} = u\parentheses*{1} &= 0
\end{align*}
or in weak formulation
\begin{quote}
    ``Find \(u \in H_0^1\parentheses*{\Omega}\) such that \(\int_0^1 u'v'\d x = \int_0^1 vf\d x\) for all \(v \in H_0^1\parentheses*{\Omega}\).''
\end{quote}

\begin{example}
    We choose a trigonometric basis, such that \(\varphi_i\parentheses*{0} = \varphi_i\parentheses*{1} = 0\), that is, \(\varphi_i\parentheses*{x} = \sin\parentheses*{i\pi x} \in H_0^1\parentheses*{\Omega}\).
    The finite dimensional subspace is \(V_N = \vecspan\braces*{\varphi_i}_{i = 1, \ldots, N} \subset H_0^1\parentheses*{\Omega}\).
    We compute for \(i, j = 1, \ldots, N\)
    \begin{align*}
        a\parentheses*{\varphi_i, \varphi_j} &= \int_0^1 \varphi_i'\parentheses*{x}\varphi_j'\parentheses*{x}\d x = \int_0^1 i\pi j\pi\cos\parentheses*{i\pi x}\cos\parentheses*{j\pi x}\d x = \begin{cases}
            \frac{i^2 \pi^2}{2}, & \text{if }i = j,\\
            0, & \text{otherwise}
        \end{cases} = a_{ij},\\
        \ell\parentheses*{\varphi_i} &= \int_0^1 \varphi_i\parentheses*{x}f\parentheses*{x} = \int_0^1 \sin\parentheses*{i\pi x}f\parentheses*{x}\d x = b_i.
    \end{align*}
    The solution has the form
    \[
        u\parentheses*{x} = \sum_{j = 1}^N \alpha_j \varphi_j\parentheses*{x} = \sum_{j = 1}^N \alpha_j \sin\parentheses*{j\pi x}
    \]
    and the unknown coefficients \(x = \parentheses*{\alpha_1, \ldots, \alpha_N}\) follow from the linear system \(Ax = b\), with
    \[
        A = \begin{pmatrix}
            \frac{pi^2}{2} & 0 & \cdots & 0\\
            0 & 2\pi^2 & \ddots & \vdots\\
            \vdots & \ddots & \ddots & 0\\
            0 & \cdots & 0 & \frac{N^2 \pi^2}{2}
        \end{pmatrix}.
    \]
    Because \(A\) is diagonal, the system is trivial to invert and gives
    \[
        \alpha_j = \frac{2b_j}{j^2 \pi^2} \implies u\parentheses*{x} = \sum_{j = 1}^N \frac{2b_j}{j^2 \pi^2}\sin\parentheses*{j\pi x}.
    \]
    This is the Fourier series.
\end{example}

\begin{example}
    We choose a polynomial basis such that \(\varphi_j\parentheses*{0} = \varphi_j\parentheses*{1} = 0\), that is, \(\varphi_i\parentheses*{x} = x^{i + 1} - x \in H_0^1\parentheses*{\Omega}\).
    The finite dimensional subspace is \(V_N = \vecspan\braces*{\varphi_i}_{i = 1, \ldots, N} \subset H_0^1\parentheses*{\Omega}\).
    We compute for \(i, j = 1, \ldots, N\)
    \begin{align*}
        a\parentheses*{\varphi_i, \varphi_j} &= \int_0^1 \varphi_i'\parentheses*{x}\varphi_j'\parentheses*{x}\d x = \int_0^1 \parentheses*{\parentheses*{i + 1}x^i - 1}\parentheses*{\parentheses*{j + 1}x^j - 1}\d x = \frac{ij}{1 + i + j} = a_{ij},\\
        \ell\parentheses*{\varphi_i} &= \int_0^1 \varphi_i\parentheses*{x}f\parentheses*{x} = \int_0^1 \parentheses*{x^{i + 1} - x}f\parentheses*{x}\d x = b_i.
    \end{align*}
    The solution has the form
    \[
        u\parentheses*{x} = \sum_{j = 1}^N \alpha_j \varphi_j\parentheses*{x} = \sum_{j = 1}^N \alpha_j \parentheses*{x^{j + 1} - x}
    \]
    and the unknown coefficients \(x = \parentheses*{\alpha_1, \ldots, \alpha_N}\) follow from the linear system \(Ax = b\), with
    \[
        A = \begin{pmatrix}
            \frac{1}{3} & \frac{1}{2} & \frac{3}{5} & \cdots \\
            \frac{1}{2} & \frac{4}{5} & 1 & \\
            \frac{3}{5} & 1 & \frac{9}{7} & \\
            \vdots & & & \ddots
        \end{pmatrix}.
    \]
    Because \(A\) is fully occupied the solution has a highly non-trivial structure, for instance with \(N = 3\):
    \begin{align*}
        \alpha_1 &= 300b_1 - 450b_2 + 210b_3,\\
        \alpha_2 &= -450b_1 + 720b_2 - 350b_3,\\
        \alpha_3 &= 210b_1 - 350b_2 + 175b_3.
    \end{align*}
\end{example}


\subsection{Galerkin orthogonality}

\begin{theorem}
    Let \(V, a, \ell\) be as above and we consider the variational formulation.
    The solution \(u \in V\) of the inifite problem and the solution \(u_N \in V_N\) of the finite, ``discretized'' problem satisfy
    \[
        a\parentheses*{u - u_N, v_N} = 0 \quad \forall v_N \in V_N.
    \]
\end{theorem}

\begin{proof}
    We have \(a\parentheses*{u, v} = \ell\parentheses*{v}\) for all \(v \in V\), but in particular also \(a\parentheses*{u, v_N} = \ell\parentheses*{v_N}\), because \(v_N \in V_N \subset V\).
    In the discrete setting we have \(a\parentheses*{u_N, v_N} = \ell\parentheses*{v_N}\) for all \(v_N \in V_N\) and thus
    \[
        a\parentheses*{u - u_N, v_N} = 0 \quad \forall v_N \in V_N.
    \]
\end{proof}

\begin{remark}
    \begin{enumerate}
        \item The symmetric, coercive, bilinear form can be used to define a special scalar product \(\angles*{u, v}_a := a\parentheses*{u, v}\).
        The Galerkin orthogonality is interpreted in this setting.
        The error of the approximation \(u - u_N\) is orthogonal to all elements of \(V_N\) measured in the scalar product \(\angles*{u, v}_a\).
        The approximation \(u_N\) is called an orthogonal projection of the exact \(u\) into the space \(V_N\).
        \item An associated norm is given by \(\norm*{u}_a := \sqrt{a\parentheses*{u, u}}\), which is called the ``energy-norm'' for physical reasons.
        Because of the orthogonality the approximation is the best approximation of \(u\) you can find in \(V_N\) when measuring the error \(u - u_N\) in the norm \(\norm*{\cdot}_a\).
        We have the error estimate
        \[
            \norm*{u - u_N}_a \le \min_{v_N \in V_N}\norm*{u - v_N}_a
        \]
        in the energy norm.
    \end{enumerate}
\end{remark}


\subsection{Optimal accuracy}

The question arises how accurate the approximation \(u_N\) of the variational problem in a ``usual'' norm is?

\begin{theorem}
    Let \(V, a, \ell\) be as above and we consider the variational formulation.
    The soultion \(u \in V\) of the infinite problem and the solution \(u_N \in V_N\) of the finite, ``discretized'' problem satisfy
    \[
        \norm*{u - u_N}_V \le \frac{C}{\gamma}\min_{v_N \in V_N}\norm*{u - v_N}_V
    \]
\end{theorem}

\begin{proof}
    First, we want to show that \(a\parentheses*{u - u_N, u - u_N} = a\parentheses*{u - u_N, u - v_N}\) for all \(v_N \in V_N\).
    We have Galerkin orthogonality, and in particular, \(a\parentheses*{u - u_N, u_N} = 0\), because \(u_N \in V_N\).
    Hence,
    \[
        a\parentheses*{u - u_N, u} - \underbrace{a\parentheses*{u - u_N, u_N}}_{= 0} = a\parentheses*{u - u_N, u} - \underbrace{a\parentheses*{u - u_N, v_N}}_{= 0} \implies a\parentheses*{u - u_N, u - u_N} = a\parentheses*{u - u_N, u - v_N}.
    \]
    On the one side we use coercivity and on the other continuity of the bilinear form
    \[
        \gamma\norm*{u - u_N}_V^2 \le \absolute*{a\parentheses*{u - u_N, u - u_N}} = \absolute*{a\parentheses*{u - u_N, u - v_N}} \le C\norm*{u - u_N}_V \norm*{u - v_N}_V,
    \]
    which gives
    \[
        \norm*{u - u_N}_V \le \frac{C}{\gamma}\norm*{u - v_N}_V \quad \forall v_N \in V_N.
    \]
    This means we can take the minimum with respect to \(v_N\) and achieve a minimum upper bound for the error \(\norm*{u - u_N}_V\).
\end{proof}

\begin{remark}
    \begin{enumerate}
        \item The coefficient \(\frac{C}{\gamma}\) is the only difference to the ``optimal'' estimate with the energy norm and constant unity.
        \item Consequently, the choice of the approximative space \(V_N\) is very important.
        If we have \(\lim_{N \to \infty}V_N \simeq V\), that is all functions can be represented if \(N\) is large enough, like in the trigonometric or polynomial case, the Ritz-Galerkin method converges.
        Since \(u \in V\) we have
        \[
            \min_{v_N \in V_N}\norm*{u - v_N}_V \xrightarrow{N \to \infty} 0,
        \]
        hence, \(\norm*{u - u_N} \xrightarrow{N \to \infty} 0\).
    \end{enumerate}
\end{remark}

\begin{example}
    We would like to approximate the function \(g: \Omega \to \R\) by a simpler function \(u_N: \Omega \to \R\).
    For this we consider the Ritz-Galerkin method for the variational formulation of the (tivial) equation \(u = g\)
    \begin{quote}
        ``Find \(u_N \in V_N\) such that \(\int_\Omega u_N v\d x = \int_\Omega gv\d x\) for all \(v \in V_N\).''
    \end{quote}
    or if \(\braces*{\varphi_i}_{i = 1, \ldots, N}\) is a basis of \(V_N\)
    \begin{quote}
        ``Find \(u_N = \sum_{j = 1}^N \alpha_j \varphi_j\) such that \(\int_\Omega u_N \varphi_i \d x = \int_\Omega g\varphi_i \d x\) for \(i = 1, \ldots, N\).''
    \end{quote}
    As above this gives a linear system for \(\parentheses*{\alpha_1, \ldots, \alpha_N}\) and the result is called the \(L^2\)-projection of \(g\) in \(V_N\).
    The error \(\norm*{u - u_N} = \norm*{g - u_N}\) is minimal in \(\norm*{\cdot}_{L^2}\) among any function of \(V_N\).
\end{example}
